\relax 
\providecommand \oddpage@label [2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Exercise Session 1}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}A simple example: two Gaussians}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Support vector machine classifier}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Visualizing the change in the hyperplane by adding more data points\relax }}{3}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:adding}{{1}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Varying the $c$ parameter for the linear kernel \relax }}{3}}
\newlabel{fig:c}{{2}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces An example of a kernel transformation \relax }}{4}}
\newlabel{fig:example}{{3}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3} Least-squares support vector machine classifier}{5}}
\newlabel{hyperparameters}{{1.3}{5}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.3.1} Influence of hyperparameters and kernel parameters}{5}}
\newlabel{fig:x_a}{{4a}{6}}
\newlabel{sub@fig:x_a}{{a}{6}}
\newlabel{fig:x_b}{{4b}{6}}
\newlabel{sub@fig:x_b}{{b}{6}}
\newlabel{fig:x_c}{{4c}{6}}
\newlabel{sub@fig:x_c}{{c}{6}}
\newlabel{fig:x_d}{{4d}{6}}
\newlabel{sub@fig:x_d}{{d}{6}}
\newlabel{fig:x_e}{{4e}{6}}
\newlabel{sub@fig:x_e}{{e}{6}}
\newlabel{fig:x_f}{{4f}{6}}
\newlabel{sub@fig:x_f}{{f}{6}}
\newlabel{fig:x_g}{{4g}{6}}
\newlabel{sub@fig:x_g}{{g}{6}}
\newlabel{fig:x_h}{{4h}{6}}
\newlabel{sub@fig:x_h}{{h}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Varying $c$ and $\sigma ^2$ for the RBF kernel\relax }}{6}}
\newlabel{fig:rbf}{{4}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Comparison of the decision surfaces produced by a linear (a) and an RBF (b) kernel. \relax }}{7}}
\newlabel{fig:compare}{{5}{7}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces  Varying the degree of the polynomial kernel.\relax }}{7}}
\newlabel{fig:iris}{{6}{7}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces  Prediction error on the test set is measured for a different range of values for $\sigma ^2$ (x-axis) and $\gamma $ (y-axis). \relax }}{8}}
\newlabel{fig:cgama}{{7}{8}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Varying $\sigma ^2$ and $\gamma $ parameters in the RBF kernel. In (a) the decision boundary is centered around the points of one class which impacts the generalization ability of the classifier. \relax }}{8}}
\newlabel{fig:irisrbf}{{8}{8}}
\citation{tool}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.3.2}Tuning parameters using validation}{9}}
\newlabel{tuning}{{1.3.3}{9}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.3.3}Automatic parameter tuning}{9}}
\newlabel{fig:x_a}{{9a}{10}}
\newlabel{sub@fig:x_a}{{a}{10}}
\newlabel{fig:x_b}{{9b}{10}}
\newlabel{sub@fig:x_b}{{b}{10}}
\newlabel{fig:x_c}{{9c}{10}}
\newlabel{sub@fig:x_c}{{c}{10}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Visualization of the of misclassification error as a function of parameters $\sigma ^2$ and $\gamma $ for Random split, 10-fold validation and leave-one-out validation \relax }}{10}}
\newlabel{fig:val}{{9}{10}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Hyperparameters tuned with Nelder-Mead method and Gridsearch for the classification task\relax }}{10}}
\newlabel{fig:tune1}{{1}{10}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.3.4} ROC curves}{10}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces  ROC curve for the test partition of the iris dataset \relax }}{10}}
\newlabel{fig:roc}{{10}{10}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Probabilistic interpretation of the classification boundary \relax }}{11}}
\newlabel{fig:bay}{{11}{11}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.3.5}Bayesian framework}{11}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4}Homework Problems}{11}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.4.1}Classification of the Ripley dataset}{11}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Classifying the Ripley dataset\relax }}{12}}
\newlabel{fig:ripley}{{12}{12}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.4.2}Classification of the The Breast Cancer dataset}{12}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Distribution of features for the Breast cancer dataset \relax }}{13}}
\newlabel{fig:br}{{13}{13}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces Classifying the Breast cancer dataset\relax }}{14}}
\newlabel{fig:brf}{{14}{14}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Classification accuracy on the Breast cancer dataset after projecting the input data to 2-30 principal components\relax }}{14}}
\newlabel{fig:brtable}{{2}{14}}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces Distribution of features for the diabetes dataset\relax }}{15}}
\newlabel{fig:featuresdiab}{{15}{15}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Classification accuracy on the Diabetes dataset after projecting the input data to 2-8 principal components\relax }}{15}}
\newlabel{fig:diabable}{{3}{15}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.5}Classification of the Diabetes dataset}{15}}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces  ROC curve for the test partition of the diabetes dataset \relax }}{16}}
\newlabel{fig:rocdiab}{{16}{16}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Exercise Session 2}{17}}
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces An example where the linear kernel is the best choice for the function approximation task\relax }}{18}}
\newlabel{fig:reglin}{{17}{18}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Regression of the sinc function}{18}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Observing the behavior of hyperparameters}{18}}
\@writefile{lof}{\contentsline {figure}{\numberline {18}{\ignorespaces As the value of c decreases the approximation becomes more smooth and the model more sparse.\relax }}{19}}
\newlabel{fig:bound}{{18}{19}}
\@writefile{lof}{\contentsline {figure}{\numberline {19}{\ignorespaces As the value of $\epsilon $ decreases the approximation becomes more smooth\relax }}{20}}
\newlabel{fig:ep}{{19}{20}}
\newlabel{fig:4}{{20a}{21}}
\newlabel{sub@fig:4}{{a}{21}}
\newlabel{fig:5}{{20b}{21}}
\newlabel{sub@fig:5}{{b}{21}}
\newlabel{fig:6}{{20c}{21}}
\newlabel{sub@fig:6}{{c}{21}}
\newlabel{fig:4}{{20d}{21}}
\newlabel{sub@fig:4}{{d}{21}}
\newlabel{fig:5}{{20e}{21}}
\newlabel{sub@fig:5}{{e}{21}}
\newlabel{fig:6}{{20f}{21}}
\newlabel{sub@fig:6}{{f}{21}}
\newlabel{fig:1}{{20g}{21}}
\newlabel{sub@fig:1}{{g}{21}}
\newlabel{fig:2}{{20h}{21}}
\newlabel{sub@fig:2}{{h}{21}}
\newlabel{fig:3}{{20i}{21}}
\newlabel{sub@fig:3}{{i}{21}}
\@writefile{lof}{\contentsline {figure}{\numberline {20}{\ignorespaces Trying different $\gamma $ values for $\sigma = 0.1$\relax }}{21}}
\newlabel{fig:regam}{{20}{21}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Tuning the hyperparameters}{21}}
\newlabel{fig:4}{{21a}{22}}
\newlabel{sub@fig:4}{{a}{22}}
\newlabel{fig:5}{{21b}{22}}
\newlabel{sub@fig:5}{{b}{22}}
\newlabel{fig:6}{{21c}{22}}
\newlabel{sub@fig:6}{{c}{22}}
\newlabel{fig:4}{{21d}{22}}
\newlabel{sub@fig:4}{{d}{22}}
\newlabel{fig:5}{{21e}{22}}
\newlabel{sub@fig:5}{{e}{22}}
\newlabel{fig:6}{{21f}{22}}
\newlabel{sub@fig:6}{{f}{22}}
\newlabel{fig:1}{{21g}{22}}
\newlabel{sub@fig:1}{{g}{22}}
\newlabel{fig:2}{{21h}{22}}
\newlabel{sub@fig:2}{{h}{22}}
\newlabel{fig:sig2}{{21i}{22}}
\newlabel{sub@fig:sig2}{{i}{22}}
\@writefile{lof}{\contentsline {figure}{\numberline {21}{\ignorespaces Trying different $\sigma ^2$ values for $\gamma = 1000$\relax }}{22}}
\newlabel{fig:regsig2}{{21}{22}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Application of the Bayesian framework}{22}}
\@writefile{lof}{\contentsline {figure}{\numberline {22}{\ignorespaces Observing the distribution of the tuned parameters\relax }}{23}}
\newlabel{fig:tune3}{{22}{23}}
\@writefile{lof}{\contentsline {figure}{\numberline {23}{\ignorespaces Kernel Density estimate\relax }}{23}}
\newlabel{fig:tune4}{{23}{23}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Hyperparameters tuned with Nelder-Mead method and Gridsearch for the function approximation task\relax }}{24}}
\newlabel{table:tune2}{{4}{24}}
\@writefile{lof}{\contentsline {figure}{\numberline {24}{\ignorespaces Applying the Bayesian framework for the function approximation task\relax }}{25}}
\newlabel{fig:bayes}{{24}{25}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}Automatic Relevance Determination}{25}}
\citation{santafe}
\@writefile{lof}{\contentsline {figure}{\numberline {25}{\ignorespaces  ROC curve for the test partition of the diabetes dataset \relax }}{26}}
\newlabel{fig:aut}{{25}{26}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6}Robust regression}{26}}
\@writefile{lof}{\contentsline {figure}{\numberline {26}{\ignorespaces Applying the unweighted LS-SVM model for the task of regression\relax }}{27}}
\newlabel{fig:rob1}{{26}{27}}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces The Huber, Hampel, Logistic and Myriad functions.\relax }}{27}}
\newlabel{table:rob}{{5}{27}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.7}Time Series Prediction}{27}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.7.1}Santa Fe dataset}{27}}
\@writefile{lof}{\contentsline {figure}{\numberline {27}{\ignorespaces Visualizing the regression results of the weighted LS-SVM for different weighting functions\relax }}{28}}
\newlabel{fig:robres2}{{27}{28}}
\@writefile{lof}{\contentsline {figure}{\numberline {28}{\ignorespaces Santa Fe Time Series Prediction\relax }}{29}}
\newlabel{fig:santa}{{28}{29}}
\@writefile{lof}{\contentsline {figure}{\numberline {29}{\ignorespaces Prediction on the Log map dataset\relax }}{29}}
\newlabel{fig:logmap}{{29}{29}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.7.2}Logmap dataset}{29}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Exercise Session 3}{30}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Kernel principal component analysis}{30}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Spectral clustering}{30}}
\@writefile{lof}{\contentsline {figure}{\numberline {30}{\ignorespaces Linear PCA and kernel PCA on a two dimensional dataset\relax }}{31}}
\newlabel{fig:kpca1}{{30}{31}}
\@writefile{lof}{\contentsline {figure}{\numberline {31}{\ignorespaces Varying the number of components in kerle PCA\relax }}{31}}
\newlabel{fig:kpca2}{{31}{31}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Fixed-size LS-SVM}{32}}
\citation{nystrom}
\newlabel{fig:4}{{\caption@xref {fig:4}{ on input line 1415}}{33}}
\newlabel{sub@fig:4}{{}{33}}
\newlabel{fig:5}{{\caption@xref {fig:5}{ on input line 1420}}{33}}
\newlabel{sub@fig:5}{{}{33}}
\newlabel{fig:6}{{\caption@xref {fig:6}{ on input line 1425}}{33}}
\newlabel{sub@fig:6}{{}{33}}
\newlabel{fig:1}{{32a}{33}}
\newlabel{sub@fig:1}{{a}{33}}
\newlabel{fig:2}{{32b}{33}}
\newlabel{sub@fig:2}{{b}{33}}
\newlabel{fig:3}{{32c}{33}}
\newlabel{sub@fig:3}{{c}{33}}
\@writefile{lof}{\contentsline {figure}{\numberline {32}{\ignorespaces Varying $\sigma ^2$ in spectral clustering\relax }}{33}}
\newlabel{fig:spec1}{{32}{33}}
\citation{l0}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Image denoising using kernel PCA}{34}}
\newlabel{fig:4}{{\caption@xref {fig:4}{ on input line 1486}}{35}}
\newlabel{sub@fig:4}{{}{35}}
\newlabel{fig:5}{{\caption@xref {fig:5}{ on input line 1490}}{35}}
\newlabel{sub@fig:5}{{}{35}}
\newlabel{fig:6}{{\caption@xref {fig:6}{ on input line 1494}}{35}}
\newlabel{sub@fig:6}{{}{35}}
\@writefile{lof}{\contentsline {figure}{\numberline {33}{\ignorespaces Comparing fixed size LS-SVM with L0-LS-SVM for k=5\relax }}{35}}
\newlabel{fig:1}{{\caption@xref {fig:1}{ on input line 1502}}{35}}
\newlabel{sub@fig:1}{{}{35}}
\newlabel{fig:2}{{\caption@xref {fig:2}{ on input line 1507}}{35}}
\newlabel{sub@fig:2}{{}{35}}
\newlabel{fig:3}{{\caption@xref {fig:3}{ on input line 1512}}{35}}
\newlabel{sub@fig:3}{{}{35}}
\@writefile{lof}{\contentsline {figure}{\numberline {34}{\ignorespaces Comparing fixed size LS-SVM with L0-LS-SVM when k=10\relax }}{35}}
\newlabel{fig:ff}{{34}{35}}
\@writefile{lof}{\contentsline {figure}{\numberline {35}{\ignorespaces Entropy as a function of $\sigma ^2$ in the Fixed LS-SVM\relax }}{36}}
\newlabel{fig:fixed}{{35}{36}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5}Fixed Size LS-SVM}{36}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.5.1}Classification on the Shuttle dataset}{36}}
\@writefile{lof}{\contentsline {figure}{\numberline {36}{\ignorespaces Comparing reconstructions of linear and kernel PCA\relax }}{37}}
\newlabel{fig:digits1}{{36}{37}}
\@writefile{lot}{\contentsline {table}{\numberline {6}{\ignorespaces The StatLog shuttle dataset\relax }}{37}}
\newlabel{ref:shuttle}{{6}{37}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.5.2}Regression on the California housing dataset}{37}}
\newlabel{fig:4}{{37a}{38}}
\newlabel{sub@fig:4}{{a}{38}}
\newlabel{fig:5}{{37b}{38}}
\newlabel{sub@fig:5}{{b}{38}}
\newlabel{fig:6}{{37c}{38}}
\newlabel{sub@fig:6}{{c}{38}}
\newlabel{fig:1}{{37d}{38}}
\newlabel{sub@fig:1}{{d}{38}}
\@writefile{lof}{\contentsline {figure}{\numberline {37}{\ignorespaces Varying $\sigma ^2$ in image denoising\relax }}{38}}
\newlabel{fig:kpcad1}{{37}{38}}
\@writefile{lot}{\contentsline {table}{\numberline {7}{\ignorespaces The California house dataset\relax }}{38}}
\newlabel{ref:california}{{7}{38}}
\newlabel{fig:4}{{38a}{39}}
\newlabel{sub@fig:4}{{a}{39}}
\newlabel{fig:5}{{38b}{39}}
\newlabel{sub@fig:5}{{b}{39}}
\newlabel{fig:6}{{38c}{39}}
\newlabel{sub@fig:6}{{c}{39}}
\newlabel{fig:1}{{38d}{39}}
\newlabel{sub@fig:1}{{d}{39}}
\@writefile{lof}{\contentsline {figure}{\numberline {38}{\ignorespaces RMSE as a function of $\sigma ^2$ in image denoising\relax }}{39}}
\newlabel{fig:kpcad3}{{38}{39}}
\newlabel{fig:4}{{39a}{39}}
\newlabel{sub@fig:4}{{a}{39}}
\newlabel{fig:5}{{39b}{39}}
\newlabel{sub@fig:5}{{b}{39}}
\@writefile{lof}{\contentsline {figure}{\numberline {39}{\ignorespaces RMSE on the train and validation sets as a function of different principal components\relax }}{39}}
\newlabel{fig:kpcad5}{{39}{39}}
\@writefile{lof}{\contentsline {figure}{\numberline {40}{\ignorespaces RMSE on the test and validation sets \relax }}{40}}
\newlabel{fig:kpca6}{{40}{40}}
\newlabel{fig:4}{{41a}{40}}
\newlabel{sub@fig:4}{{a}{40}}
\newlabel{fig:5}{{41b}{40}}
\newlabel{sub@fig:5}{{b}{40}}
\newlabel{fig:5}{{41c}{40}}
\newlabel{sub@fig:5}{{c}{40}}
\@writefile{lof}{\contentsline {figure}{\numberline {41}{\ignorespaces Classification results on the The StatLog shuttle dataset \relax }}{40}}
\newlabel{fig:shuttle}{{41}{40}}
\@writefile{lof}{\contentsline {figure}{\numberline {42}{\ignorespaces The features of California Housing Dataset\relax }}{41}}
\newlabel{fig:calidist}{{42}{41}}
\newlabel{fig:4}{{\caption@xref {fig:4}{ on input line 1807}}{42}}
\newlabel{sub@fig:4}{{}{42}}
\newlabel{fig:5}{{\caption@xref {fig:5}{ on input line 1811}}{42}}
\newlabel{sub@fig:5}{{}{42}}
\newlabel{fig:6}{{\caption@xref {fig:6}{ on input line 1815}}{42}}
\newlabel{sub@fig:6}{{}{42}}
\@writefile{lof}{\contentsline {figure}{\numberline {43}{\ignorespaces Classification of the California dataset for linear kernel and $k=1$\relax }}{42}}
\newlabel{fig:1}{{\caption@xref {fig:1}{ on input line 1823}}{42}}
\newlabel{sub@fig:1}{{}{42}}
\newlabel{fig:2}{{\caption@xref {fig:2}{ on input line 1828}}{42}}
\newlabel{sub@fig:2}{{}{42}}
\newlabel{fig:3}{{\caption@xref {fig:3}{ on input line 1833}}{42}}
\newlabel{sub@fig:3}{{}{42}}
\@writefile{lof}{\contentsline {figure}{\numberline {44}{\ignorespaces Classification of the California dataset for linear kernel and $k=5$\relax }}{42}}
\newlabel{fig:cal1}{{44}{42}}
\bibstyle{IEEEtran}
\bibdata{fdr_bib}
\bibcite{santafe}{1}
\bibcite{regression}{2}
\bibcite{tool}{3}
\bibcite{nelder}{4}
\bibcite{bayesian}{5}
\bibcite{timeseries}{6}
\bibcite{santafe}{7}
\bibcite{fixed}{8}
\bibcite{nystrom}{9}
\bibcite{l0}{10}
\bibcite{}{11}
\@writefile{toc}{\contentsline {section}{References}{43}}
