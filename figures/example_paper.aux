\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:adding}{{1}{2}{Adding more points to the dataset on the right side of the hyperplane (first row) and on the wrong side of the hyperplane (second row). Adding points on the wrong side of the hyperplane drastically changes the hyperplane, as opposed to adding points on the right side of the plane which has a minor effect.\relax }{figure.caption.1}{}}
\newlabel{fig:c}{{2}{2}{Varying the $c$ parameter for the linear kernel. As c is increased from 0.1 (a) to 1.0 (b) and 100 (c) the hyperplane is adjusted and the margin becomes smaller as fewer misclassifications are tolerated. \relax }{figure.caption.2}{}}
\newlabel{fig:example}{{3}{3}{In the left plot, the original data points are shown which are not linearly separable in the 1-dimensional space. After applying the transformation $\phi (x)=x^2$ a second dimension is added to the feature space and the classes become linearly separable \relax }{figure.caption.3}{}}
\newlabel{fig:rbf}{{1.2}{3}{}{figure.caption.4}{}}
\newlabel{fig:x_a}{{4a}{4}{$c=0.032, s=0.32$\relax }{figure.caption.4}{}}
\newlabel{sub@fig:x_a}{{a}{4}{$c=0.032, s=0.32$\relax }{figure.caption.4}{}}
\newlabel{fig:x_b}{{4b}{4}{$c=1.3, s=0.32$\relax }{figure.caption.4}{}}
\newlabel{sub@fig:x_b}{{b}{4}{$c=1.3, s=0.32$\relax }{figure.caption.4}{}}
\newlabel{fig:x_c}{{4c}{4}{$c=20, s=0.32$\relax }{figure.caption.4}{}}
\newlabel{sub@fig:x_c}{{c}{4}{$c=20, s=0.32$\relax }{figure.caption.4}{}}
\newlabel{fig:x_d}{{4d}{4}{$c=63, s=0.32$\relax }{figure.caption.4}{}}
\newlabel{sub@fig:x_d}{{d}{4}{$c=63, s=0.32$\relax }{figure.caption.4}{}}
\newlabel{fig:x_e}{{4e}{4}{$c=10, s=0.13$\relax }{figure.caption.4}{}}
\newlabel{sub@fig:x_e}{{e}{4}{$c=10, s=0.13$\relax }{figure.caption.4}{}}
\newlabel{fig:x_f}{{4f}{4}{$c=20, s=0.43$\relax }{figure.caption.4}{}}
\newlabel{sub@fig:x_f}{{f}{4}{$c=20, s=0.43$\relax }{figure.caption.4}{}}
\newlabel{fig:x_g}{{4g}{4}{$c=10, s=1.6$\relax }{figure.caption.4}{}}
\newlabel{sub@fig:x_g}{{g}{4}{$c=10, s=1.6$\relax }{figure.caption.4}{}}
\newlabel{fig:x_h}{{4h}{4}{$c=10, s=20$\relax }{figure.caption.4}{}}
\newlabel{sub@fig:x_h}{{h}{4}{$c=10, s=20$\relax }{figure.caption.4}{}}
\newlabel{fig:compare}{{5}{5}{Comparison of the decision surfaces produced by a linear (a) and an RBF (b) kernel. \relax }{figure.caption.5}{}}
\newlabel{fig:cgama}{{6}{6}{Prediction error on the test set is measured for a different range of values for $\sigma $ (x-axis) and $\gamma $ (y-axis). The figure suggests that a possible range of values for the two parameters is $\sigma \in [0.05,5], \gamma \in [1,10]$. \relax }{figure.caption.6}{}}
\newlabel{fig:iris}{{7}{7}{Varying the degree of the polynomial kernel. A linear kernel is unable to properly separate the data points. Increasing the degree of the polynomial kernel results in more expressive decision surfaces. However, if the degree of the polynomial kernel is too high this results in overfitting.\relax }{figure.caption.7}{}}
\newlabel{fig:irisrbf}{{8}{8}{Varying $\sigma $ and $\gamma $ parameters in the RBF kernel. In (a) the decision boundary is centered around the points of one class which impacts the generalization ability of the classifier. In (d) \relax }{figure.caption.8}{}}
\newlabel{fig:x_a}{{9a}{8}{Random split\relax }{figure.caption.9}{}}
\newlabel{sub@fig:x_a}{{a}{8}{Random split\relax }{figure.caption.9}{}}
\newlabel{fig:x_b}{{9b}{8}{10-fold cross validation\relax }{figure.caption.9}{}}
\newlabel{sub@fig:x_b}{{b}{8}{10-fold cross validation\relax }{figure.caption.9}{}}
\newlabel{fig:x_c}{{9c}{8}{leave-one-out cross validation \relax }{figure.caption.9}{}}
\newlabel{sub@fig:x_c}{{c}{8}{leave-one-out cross validation \relax }{figure.caption.9}{}}
\newlabel{fig:val}{{9}{8}{Visualization of the of misclassification error as a function of parameters $\sigma ^2$ and $\gamma $ for Random split, 10-fold validation and leave-one-out validation \relax }{figure.caption.9}{}}
\newlabel{fig:roc}{{10}{9}{ROC curve for the test partition of the iris dataset \relax }{figure.caption.11}{}}
\newlabel{fig:tune1}{{1}{10}{Hyperparameters tuned with Nelder-Mead method and Brute Force Gridsearch \relax }{table.caption.10}{}}
\newlabel{fig:ripley}{{11}{10}{Classifying the Ripley dataset\relax }{figure.caption.12}{}}
\newlabel{fig:brtable}{{2}{11}{Classification accuracy on the Breast cancer dataset after projecting the input data to 2-30 principal components\relax }{table.caption.13}{}}
\newlabel{fig:br}{{12}{12}{Distribution of features for the Breast cancer dataset \relax }{figure.caption.14}{}}
\newlabel{fig:brf}{{13}{13}{Classifying the Breast cancer dataset\relax }{figure.caption.15}{}}
\newlabel{fig:featuresdiab}{{14}{15}{Distribution of features for the diabetes dataset\relax }{figure.caption.16}{}}
